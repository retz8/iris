# IRIS Strategy — Session Output (2026-02-10)

> Output of a product strategy session between Product Manager, YC Partner (advisor), Senior Engineer, and Engineering Manager.

---

## Identity

**One-liner:** IRIS maps the structure of code you didn't write — so you can review, verify, and navigate it in seconds.

**Elevator pitch:** AI is writing more code than ever. Before you can verify it, you need to understand it. IRIS gives you an instant structural map of any file — its purpose, its major components, and how they relate. So you can verify, review, and navigate code you didn't write — whether it came from Copilot, a teammate, or an open-source project.

**What IRIS is:** A comprehension tool — NOT a verification/bug-finding tool. The verification bottleneck is the "why now" narrative, not the product description.

**Why now:** AI code generation is exploding. Every Copilot/Cursor suggestion accepted is code that needs to be understood by a human. The comprehension bottleneck is the fastest-growing developer productivity problem.

---

## Competitive Position

| Tool | Role | Moment | Surface |
|------|------|--------|---------|
| Copilot/Cursor | Writes code | Generation | Editor inline |
| Greptile/CodeRabbit | Judges code (finds bugs) | PR review | PR comments |
| **IRIS** | **Maps code structure** | **Pre-judgment comprehension** | **Editor overlay** |

IRIS is the comprehension layer between code generation and code review. Not competing with Greptile — completing the stack.

> "Greptile tells you what might be wrong. IRIS helps you understand the code well enough to know if they're right — and catch what they missed."

**Why incumbents won't build this:**
- Copilot optimizes for generation speed — "slow down and verify" is antithetical
- Greptile optimizes for automated review — in-editor reading aid is orthogonal
- Cursor builds an AI-native IDE — structural verification overlay doesn't fit their "AI does everything" narrative
- IRIS is for the human in the loop. Everyone else is trying to remove the human.

---

## Target Personas (priority order)

1. **AI-assisted developer** — verifies Copilot/Cursor output daily. Largest group, highest frequency. *Leads marketing.*
2. **Code reviewer** — reviews PRs with unfamiliar code. Medium frequency, clear workflow insertion.
3. **New joiner** — onboards to unfamiliar codebase. Most acute pain, time-bounded.

All three share the same action: "I need to understand code I didn't write, fast."

---

## 6-Week Execution Plan

### Week 1: Ship

- Marketplace prep: publisher ID, icon, activation events, first-run experience (3-5 days)
- Expand language support to all LLM-supported languages (1 hour)
- Verification-mode prompt: risk-ordered blocks, `verification_hint` per block (start, finish by week 2)
- Auto-analyze set to **manual trigger by default** (cleaner retention signal during validation)
- Ship verification framing ONLY — no dual modes, no toggles. Test one thesis.

### Week 2: Validate

- Publish to VS Code Marketplace
- Opt-in telemetry: installs, analyses, D1/D7 retention, language
- Recruit 10 non-friend developers (preferably Copilot/Cursor users)
- Screen-share observation sessions — watch them use it, don't prompt
- Start content: "We asked Claude to build an API. Here's what IRIS revealed."

### Week 3-5: Build + Distribute

- **Diff-aware block highlighting** (3-4 weeks): Cross-reference git diff with responsibility blocks. "2 of 5 blocks modified in this PR — focus here." This is the feature that makes verification positioning real in the product.
- Content distribution: HN Show, Reddit r/programming, dev Twitter
- OSS maintainer outreach: 10 mid-size projects, offer IRIS maps for contributor onboarding

### Week 4: Decision Point

| D7 Retention | Action |
|---|---|
| > 15% | Double down. Accelerate PR bot + import graph. |
| 5-15% | Interview churned users. Find what's missing. |
| < 5% | Value prop needs to change. Pivot before building more. |

### Week 5-6: PR Bot + Import Graph

- PR bot prototype: IRIS maps as GitHub PR comments (top-of-funnel, zero friction)
- Import-based "related files" via Tree-sitter (2-3 weeks, uses existing `extract_import_details()`)
- Test PR bot with 10 teams on real PRs
- Gather signal: do reviewers feel more confident about AI-generated code?

---

## Post-Validation Roadmap (Month 2+, contingent on retention signal)

1. **Structural diff** (4-6 weeks): Before/after block-level comparison when code changes. "Before: 3 blocks. After: 5. New blocks: X and Y." Verification at the conceptual level, not line level. No competitor has this.
2. **Import graph blast radius** (2-3 weeks): "This file's changes affect 4 downstream files." Uses existing Tree-sitter parser.
3. **Persistent maps** / cross-session storage: Store in VS Code `globalState` keyed by content hash.
4. **Team features** / enterprise tier: Only after proven retention.

---

## Funnel Architecture

**Old funnel:** VS Code extension (install) → use → hope for retention
**New funnel:** PR bot (zero-friction, in existing workflow) → developer sees IRIS map → installs extension for deeper interaction

PR bot is top-of-funnel. Extension is the power-user upgrade.

---

## Distribution Strategy

1. **Content-led (weeks 2-4):** "We asked Claude to generate a REST API. Here's what IRIS revealed." Posts to HN, Reddit, dev Twitter. AI verification angle is timelier and more shareable than "IRIS analyzed React."
2. **Community-first (weeks 3-6):** Identify 10 mid-size OSS projects with onboarding problems. Generate IRIS maps for their core files. Ask maintainers to mention IRIS in CONTRIBUTING.md. Each mention is a permanent, compounding distribution channel.
3. **Workflow insertion (weeks 5-6):** PR bot posts IRIS maps as PR comments. Developers who see maps install the extension. Viral loop: Developer A has IRIS → PR gets IRIS map → Developer B sees it → installs IRIS.

---

## Business Model (Future — Do Not Gate Before Retention Signal)

- **Free tier:** Unlimited single-file analysis. Cost is negligible ($0.0008/analysis).
- **Team tier ($15-20/dev/month):** Persistent maps, cross-file graphs, shared maps, workspace-level analysis.
- **Enterprise tier ($25-30/dev/month):** Self-hosted, SSO, audit logs.
- **$1M ARR:** ~5,500 paid seats. Realistic timeline: 36+ months.

---

## What NOT to Build

- Multi-file LLM analysis (premature, 3-6 months)
- Bug finding (Greptile's lane — would produce low-quality results with single-file scope)
- AI code detection (unreliable, unnecessary — detect unfamiliarity via git signals instead)
- JetBrains plugin (after VS Code validated)
- Webview rewrite (after UI iteration needs emerge from user feedback)
- Enterprise features (after proven retention)
- Dual modes / comprehension toggle (test ONE thesis)

---

## Key Numbers

| Metric | Value |
|--------|-------|
| Cost per analysis | $0.0008 |
| Power user cost/day | $0.004–0.016 |
| Cost at 10K DAU | $160–800/month |
| Infrastructure | $8/month (t3.micro) |
| Cache hit rate | 60–80% |
| Latency (cache miss) | 1–2.5 sec |
| Latency (cache hit) | <5ms |
| Marketplace prep | 3–5 days |
| Verification prompt | 1–2 weeks |
| Diff-aware analysis | 3–4 weeks |
| Import graph (Tree-sitter) | 2–3 weeks |
| Language expansion | 1 hour |
| PR bot prototype | 2–3 weeks |

---

## YC Fundability Bar (6 weeks out)

- 500+ installs
- 15%+ D7 retention
- 5 user interviews showing clear pain
- Working PR bot prototype
- "Comprehension layer that completes the AI coding stack" positioning

> "If you come back in 6 weeks with those numbers, that's a fundable story." — YC Partner

---

## Session Participants

- **Engineering Manager** (user) — provided the verification bottleneck reframe that reshaped the strategy
- **Product Manager** — owns strategy, positioning, distribution, and user validation
- **YC Partner** (advisor) — challenged assumptions, forced clarity on market, buyer, and moat
- **Senior Engineer** — grounded discussion in technical reality, proposed diff-aware analysis and import graph
