"""Critic Agent for IRIS two-agent system.

The Critic evaluates hypotheses generated by the Analyzer and provides
actionable feedback and tool suggestions.
"""

from __future__ import annotations

from typing import Optional, TYPE_CHECKING

from openai import OpenAI

from .schemas import Hypothesis, Feedback, ToolSuggestion

if TYPE_CHECKING:
    from signature_graph import SignatureGraph
    from ..debugger.debugger import AgentFlowDebugger


class CriticAgent:
    """Agent responsible for evaluating responsibility groupings.

    The Critic:
    1. Evaluates hypothesis quality
    2. Provides specific, actionable feedback
    3. Suggests tool calls when evidence is insufficient
    4. Assigns confidence score to hypothesis
    """

    def __init__(
        self,
        client: OpenAI,
        model: str = "gpt-4o-mini",
        debugger: Optional["AgentFlowDebugger"] = None,
    ) -> None:
        """Initialize Critic agent.

        Args:
            client: OpenAI client instance
            model: Model to use for evaluation
            debugger: Optional debugger for tracking LLM calls
        """
        self.client = client
        self.model = model
        self.debugger = debugger

    def evaluate(
        self,
        hypothesis: Hypothesis,
        signature_graph: "SignatureGraph",
        filename: str,
        language: str,
    ) -> Feedback:
        """Evaluate hypothesis and provide feedback.

        Args:
            hypothesis: Hypothesis to evaluate
            signature_graph: Original signature graph for context
            filename: Name of the file being analyzed
            language: Programming language

        Returns:
            Feedback with confidence score, comments, and optional tool suggestions

        Note:
            Prompts will be implemented in Phase 2
        """
        # TODO: Implement in Phase 2 (Prompt Engineering)
        # Will use CRITIC_SYSTEM_PROMPT and build_critic_prompt
        # Returns Feedback with:
        #   - confidence: float (0.0 to 1.0)
        #   - comments: str (specific, actionable feedback)
        #   - tool_suggestions: List[ToolSuggestion] (optional)
        #   - approved: bool (based on confidence threshold)
        raise NotImplementedError("evaluate will be implemented in Phase 2")
